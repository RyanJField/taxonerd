---
title: "Recognizing taxonomic entity mentions in a corpus of text documents using TaxoNERD"
author: "Nicolas Le Guillarme and Wilfried Thuiller"
output: rmarkdown::html_vignette
runtime: shiny
vignette: >
  %\VignetteIndexEntry{taxonerd}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Learning objectives

This vignette is designed to:
1. show how to install the taxonerd package for R
2. show how to recognize taxonomic entity mentions in a corpus of PDF documents using TaxoNERD models

# Install taxonerd

taxonerd is an R package that provides deep neural network models for recognizing taxonomic entity mentions in the biodiversity literature. taxonerd can find scientific names, common names, abbreviated species names and user-defined abbreviations. Detected taxon mentions can be linked to entities in a reference taxonomy (e.g. NCBI Taxonomy, GBIF Backbone, TAXREF).

You can install the released version of taxonerd from GitHub with:

```{r eval=FALSE}
install.packages("https://github.com/nleguillarme/taxonerd/releases/download/v1.5.2/taxonerd_for_R_1.5.2.tar.gz", repos=NULL)
```

Check for available releases on the project's GitHub page : https://github.com/nleguillarme/taxonerd

The taxonerd R package wraps the TaxoNERD Python package using reticulate. So to use TaxoNERD, you have to install the Python package :

```{r eval=TRUE, results='hide'}
library(taxonerd)
install.taxonerd()
```

This will install the last release of TaxoNERD in a virtual environment named r-taxonerd. If you want TaxoNERD to run on a GPU, you will have to install the package with GPU support. First, find your CUDA version :

```{bash eval=FALSE}
nvcc --version
```

TaxoNERD supports CUDA versions 11.0 to 11.6. To install TaxoNERD for CUDA 11.3 :

```{r eval=FALSE}
install.taxonerd(cuda.version="cuda113")
```

TaxoNERD is correctly installed. You are ready to go.

## Extracting taxonomic entities to index a corpus of PDF documents

First, let's load the necessary packages for this example :

```{r results='hide'}
library(taxonerd)
library(rplos)
library(comprehenr)
library(data.table)
library(dplyr)
```

In this example, we will use taxonomic entity recognition to index a corpus of PDF documents. This will allow the corpus to be searched efficiently by taxon name.

Let's begin by creating a small corpus of PDF documents.

```{r results='hide'}
corpus.dir <- "./my_corpus"
```

```{r results='hide'}
dir.create(corpus.dir, showWarnings = FALSE)
# Get DOIs for full article in PLoS One
res <- searchplos('biodiversity AND (habitat OR distribution OR survey) AND neotropics', c('id','publication_date','title'), list('subject:ecology', 'journal_key:PLoSONE','doc_type:full'), limit = 10)
# Download full-text articles in PDF format
for (doi in res$data$id) {
  pdf.url <- gsub("manuscript", "printable", full_text_urls(doi=doi))
  dest.file <- file.path(corpus.dir, paste(gsub("/", "_", doi), "pdf", sep="."))
  download.file(pdf.url, dest.file)
}
```

The corpus.dir directory now contains a set of 10 documents in PDf format.

The next step is to initialize the taxonomic entity recognition engine. For this, we will need a taxonomic entity recognition model. TaxoNERD provides two model architectures : md - a smaller architecture designed for speed - and biobert - a more complex architecture with higher accuracy. Each architecture comes in two flavors: the en_core_eco_<md, biobert> models were fine-tuned on a gold standard corpus, while the en_core_eco_weak_<md, biobert> were fine-tuned on a silver standard corpus.

Note that we strongly recommend running the biobert architecture on a GPU, as it may be prohibitively slow on CPU. Let's download and install the en_core_eco_weak_md model:

```{r results='hide'}
install.model(model="en_core_eco_weak_md", version="1.0.0")
```

In this example, we will use the en_core_eco_weak_md model with the abbreviation detector and sentencizer enabled, and we will ask the engine to link detected taxon mentions to a subset of the NCBI Taxonomy. Entity linking relies on a approximate nearest neighbors search and uses a minimum similarity threshold to filter out entities whose name is not similar enough to the textual mention. Here, we will set the minimum similarity threshold to 0.85.

```{r}
# Initialize the taxonomic NER engine
taxonerd <- init.taxonerd(model = "en_core_eco_weak_md", exclude=list("tagger", "attribute_ruler", "parser"), linker="taxref", thresh=0.85, gpu=FALSE)
```

We are now ready to run the taxonomic entity recognition engine on our corpus of documents. Note that TaxoNERD can extract taxon mentions from (almost) any document (including txt, pdf, csv, xls, jpg, png, and many other formats). All the files required to perform entity linking against a given taxonomy are downloaded and cached the first time linking is enabled. This may take some time, but it should only be done once.

```{r}
# We apply the taxonomic entity recognition engine to all the files in the corpus.dir directory.
# If you want to apply taxonomic NER to a single file, use the find.in.file function.
# If you want to apply taxonomic NER to a piece of text, use the find.in.text function.
list.of.dfs <- find.in.corpus(taxonerd, corpus.dir)
```

The find.in.corpus function returns a list of data frames (one per document). You can also write the results of taxonomic entity recognition to a directory :

```{r eval=FALSE}
dir.create("./my_annotations", showWarnings = FALSE)
list.of.paths <- find.in.corpus(ner, corpus.dir, "./my_annotations")
```

In this case, the find.in.corpus function returns a list containing the paths to the annotation files.

We will now parse the results of taxonomic entity recognition. For each taxonomic entity detected in the corpus, we build the list of documents in which the entity is mentioned. Documents are listed in descending order of relevance (i.e. where relevance is here defined as the number of occurrences of the entity in the document).

```{r}
ents.per.doc <- list()
for (doc.name in names(list.of.dfs)) {
  ents <- list.of.dfs[[doc.name]]
  if (nrow(ents) != 0) {
    taxa.info <- lapply(ents$entity, function(l) split(l[[1]], ceiling(seq_along(l[[1]]) / 3)))
    # Extract the scientific name and id of the linked entities
    taxa.info <- lapply(taxa.info, function(l) c("id"=l$`1`[[1]], "sci_name"=l$`1`[[2]]))
    taxa.info <- rbindlist(lapply(taxa.info, as.data.frame.list))
    # Calculate the frequency of each entity
    ents.per.doc[[doc.name]] <- as.data.frame(taxa.info%>%group_by_all%>%count)
    ents.per.doc[[doc.name]]$doc_name <- doc.name
  }
}
# Aggregate results over all documents
agg.df <- do.call(rbind, ents.per.doc)
rownames(agg.df) <- NULL
unames <- unique(agg.df$sci_name)
docs.per.taxon <- lapply(unames, function(x) {
  y <- agg.df[agg.df$sci_name==x, ]
  y <- subset(y, select=-c(id, sci_name))
  y <- y[order(y$n, decreasing = TRUE),]
  rownames(y) <- NULL
  return(y)
})
names(docs.per.taxon) <- unames
```

Now that our corpus is indexed, we can look for the most relevant documents mentioning a specific taxon. For instance, we can look for all the documents mentioning *Leopardus pardalis* :

```{r}
docs.per.taxon$`Leopardus pardalis`
```